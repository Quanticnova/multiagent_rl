train start... 
steps: 2475, episodes: 100, mean episode reward: -1413.151, reward: -32.611, time: 6.552, TD error: 38.059, c_model: 19.294, actorQ: 1.177, a_model: 1.24
steps: 4975, episodes: 200, mean episode reward: -1204.587, reward: -28.602, time: 6.398, TD error: 36.736, c_model: 18.63, actorQ: 1.174, a_model: 1.214
steps: 7475, episodes: 300, mean episode reward: -1201.812, reward: -30.348, time: 6.465, TD error: 35.694, c_model: 18.106, actorQ: 1.189, a_model: 1.192
steps: 9975, episodes: 400, mean episode reward: -1214.533, reward: -31.142, time: 6.355, TD error: 35.079, c_model: 17.796, actorQ: 1.236, a_model: 1.179
steps: 12475, episodes: 500, mean episode reward: -1209.233, reward: -31.653, time: 6.444, TD error: 34.512, c_model: 17.513, actorQ: 1.316, a_model: 1.167
steps: 14975, episodes: 600, mean episode reward: -1325.98, reward: -33.003, time: 6.496, TD error: 34.056, c_model: 17.291, actorQ: 1.419, a_model: 1.157
steps: 17475, episodes: 700, mean episode reward: -1262.877, reward: -29.981, time: 6.475, TD error: 33.532, c_model: 17.043, actorQ: 1.562, a_model: 1.147
steps: 19975, episodes: 800, mean episode reward: -1038.412, reward: -20.37, time: 6.448, TD error: 33.146, c_model: 16.864, actorQ: 1.677, a_model: 1.14
steps: 22475, episodes: 900, mean episode reward: -735.536, reward: -10.452, time: 6.445, TD error: 32.688, c_model: 16.649, actorQ: 1.785, a_model: 1.13
steps: 24975, episodes: 1000, mean episode reward: -656.752, reward: -9.102, time: 6.416, TD error: 32.164, c_model: 16.401, actorQ: 1.882, a_model: 1.113
steps: 27475, episodes: 1100, mean episode reward: -692.604, reward: -9.502, time: 6.705, TD error: 31.503, c_model: 16.083, actorQ: 1.968, a_model: 1.093
steps: 29975, episodes: 1200, mean episode reward: -679.076, reward: -9.389, time: 6.907, TD error: 30.919, c_model: 15.802, actorQ: 2.049, a_model: 1.073
steps: 32475, episodes: 1300, mean episode reward: -739.907, reward: -10.692, time: 6.769, TD error: 30.208, c_model: 15.458, actorQ: 2.141, a_model: 1.047
steps: 34975, episodes: 1400, mean episode reward: -865.257, reward: -14.086, time: 7.018, TD error: 29.656, c_model: 15.191, actorQ: 2.21, a_model: 1.025
steps: 37475, episodes: 1500, mean episode reward: -1069.957, reward: -21.99, time: 6.499, TD error: 29.199, c_model: 14.969, actorQ: 2.276, a_model: 1.006
steps: 39975, episodes: 1600, mean episode reward: -1061.038, reward: -21.469, time: 6.564, TD error: 28.781, c_model: 14.767, actorQ: 2.342, a_model: 0.989
steps: 42475, episodes: 1700, mean episode reward: -880.046, reward: -14.761, time: 6.457, TD error: 28.407, c_model: 14.586, actorQ: 2.404, a_model: 0.971
steps: 44975, episodes: 1800, mean episode reward: -880.746, reward: -13.813, time: 6.474, TD error: 28.034, c_model: 14.404, actorQ: 2.464, a_model: 0.954
steps: 47475, episodes: 1900, mean episode reward: -1049.947, reward: -19.147, time: 6.63, TD error: 27.576, c_model: 14.181, actorQ: 2.536, a_model: 0.932
steps: 49975, episodes: 2000, mean episode reward: -1117.543, reward: -21.64, time: 7.408, TD error: 27.248, c_model: 14.021, actorQ: 2.592, a_model: 0.916
steps: 52475, episodes: 2100, mean episode reward: -971.66, reward: -18.075, time: 7.243, TD error: 26.951, c_model: 13.877, actorQ: 2.647, a_model: 0.9
steps: 54975, episodes: 2200, mean episode reward: -890.398, reward: -15.392, time: 6.622, TD error: 26.662, c_model: 13.736, actorQ: 2.701, a_model: 0.884
steps: 57475, episodes: 2300, mean episode reward: -853.155, reward: -14.149, time: 6.561, TD error: 26.377, c_model: 13.596, actorQ: 2.754, a_model: 0.869
steps: 59975, episodes: 2400, mean episode reward: -949.226, reward: -16.577, time: 6.565, TD error: 26.108, c_model: 13.464, actorQ: 2.807, a_model: 0.854
steps: 62475, episodes: 2500, mean episode reward: -1069.148, reward: -21.0, time: 6.884, TD error: 25.775, c_model: 13.301, actorQ: 2.872, a_model: 0.836
steps: 64975, episodes: 2600, mean episode reward: -1196.621, reward: -26.015, time: 7.07, TD error: 25.515, c_model: 13.173, actorQ: 2.923, a_model: 0.823
steps: 67475, episodes: 2700, mean episode reward: -1260.531, reward: -29.367, time: 6.543, TD error: 25.309, c_model: 13.073, actorQ: 2.975, a_model: 0.81
steps: 69975, episodes: 2800, mean episode reward: -1231.139, reward: -28.918, time: 7.645, TD error: 25.105, c_model: 12.973, actorQ: 3.027, a_model: 0.798
steps: 72475, episodes: 2900, mean episode reward: -1285.2, reward: -31.361, time: 6.902, TD error: 24.912, c_model: 12.878, actorQ: 3.079, a_model: 0.787
steps: 74975, episodes: 3000, mean episode reward: -1240.447, reward: -31.255, time: 6.472, TD error: 24.727, c_model: 12.787, actorQ: 3.131, a_model: 0.777
steps: 77475, episodes: 3100, mean episode reward: -1232.933, reward: -31.213, time: 6.573, TD error: 24.52, c_model: 12.686, actorQ: 3.196, a_model: 0.765
steps: 79975, episodes: 3200, mean episode reward: -1269.082, reward: -31.663, time: 6.555, TD error: 24.379, c_model: 12.616, actorQ: 3.248, a_model: 0.756
steps: 82475, episodes: 3300, mean episode reward: -1183.133, reward: -29.658, time: 6.52, TD error: 24.23, c_model: 12.543, actorQ: 3.3, a_model: 0.748
steps: 84975, episodes: 3400, mean episode reward: -1182.413, reward: -27.907, time: 6.441, TD error: 24.079, c_model: 12.468, actorQ: 3.352, a_model: 0.739
steps: 87475, episodes: 3500, mean episode reward: -1185.904, reward: -27.391, time: 6.372, TD error: 23.931, c_model: 12.396, actorQ: 3.404, a_model: 0.731
steps: 89975, episodes: 3600, mean episode reward: -1215.079, reward: -27.934, time: 6.369, TD error: 23.811, c_model: 12.337, actorQ: 3.456, a_model: 0.724
steps: 92475, episodes: 3700, mean episode reward: -1467.505, reward: -37.1, time: 6.574, TD error: 23.663, c_model: 12.265, actorQ: 3.522, a_model: 0.715
steps: 94975, episodes: 3800, mean episode reward: -1528.203, reward: -38.882, time: 6.383, TD error: 23.552, c_model: 12.211, actorQ: 3.575, a_model: 0.707
steps: 97475, episodes: 3900, mean episode reward: -1468.417, reward: -37.746, time: 6.348, TD error: 23.451, c_model: 12.163, actorQ: 3.629, a_model: 0.701
steps: 99975, episodes: 4000, mean episode reward: -1464.339, reward: -37.76, time: 6.477, TD error: 23.354, c_model: 12.116, actorQ: 3.682, a_model: 0.694
steps: 102475, episodes: 4100, mean episode reward: -1376.757, reward: -35.534, time: 6.4, TD error: 23.249, c_model: 12.066, actorQ: 3.735, a_model: 0.687
steps: 104975, episodes: 4200, mean episode reward: -1090.275, reward: -20.811, time: 6.47, TD error: 23.14, c_model: 12.013, actorQ: 3.788, a_model: 0.68
steps: 107475, episodes: 4300, mean episode reward: -1481.467, reward: -38.465, time: 6.544, TD error: 23.026, c_model: 11.957, actorQ: 3.855, a_model: 0.672
steps: 109975, episodes: 4400, mean episode reward: -1465.397, reward: -38.022, time: 6.397, TD error: 22.939, c_model: 11.915, actorQ: 3.908, a_model: 0.665
steps: 112475, episodes: 4500, mean episode reward: -1296.235, reward: -30.344, time: 6.495, TD error: 22.849, c_model: 11.871, actorQ: 3.961, a_model: 0.659
steps: 114975, episodes: 4600, mean episode reward: -1425.573, reward: -34.911, time: 6.667, TD error: 22.773, c_model: 11.834, actorQ: 4.014, a_model: 0.653
steps: 117475, episodes: 4700, mean episode reward: -1390.821, reward: -33.728, time: 6.498, TD error: 22.679, c_model: 11.788, actorQ: 4.066, a_model: 0.646
steps: 119975, episodes: 4800, mean episode reward: -1469.622, reward: -36.907, time: 6.492, TD error: 22.587, c_model: 11.742, actorQ: 4.119, a_model: 0.64
steps: 122475, episodes: 4900, mean episode reward: -1358.212, reward: -34.388, time: 6.46, TD error: 22.482, c_model: 11.69, actorQ: 4.184, a_model: 0.633
steps: 124975, episodes: 5000, mean episode reward: -1289.91, reward: -31.268, time: 6.386, TD error: 22.406, c_model: 11.652, actorQ: 4.236, a_model: 0.627
steps: 127475, episodes: 5100, mean episode reward: -1470.864, reward: -37.032, time: 6.426, TD error: 22.323, c_model: 11.611, actorQ: 4.289, a_model: 0.621
steps: 129975, episodes: 5200, mean episode reward: -1264.108, reward: -30.251, time: 6.614, TD error: 22.233, c_model: 11.566, actorQ: 4.341, a_model: 0.615
steps: 132475, episodes: 5300, mean episode reward: -1356.157, reward: -31.083, time: 6.7, TD error: 22.155, c_model: 11.526, actorQ: 4.393, a_model: 0.61
steps: 134975, episodes: 5400, mean episode reward: -1462.38, reward: -35.036, time: 6.859, TD error: 22.066, c_model: 11.481, actorQ: 4.445, a_model: 0.604
steps: 137475, episodes: 5500, mean episode reward: -1332.047, reward: -30.499, time: 6.502, TD error: 21.976, c_model: 11.435, actorQ: 4.509, a_model: 0.597
steps: 139975, episodes: 5600, mean episode reward: -1361.033, reward: -32.968, time: 6.63, TD error: 21.894, c_model: 11.394, actorQ: 4.561, a_model: 0.592
steps: 142475, episodes: 5700, mean episode reward: -1382.855, reward: -34.347, time: 6.987, TD error: 21.823, c_model: 11.357, actorQ: 4.613, a_model: 0.586
steps: 144975, episodes: 5800, mean episode reward: -1294.242, reward: -29.71, time: 7.008, TD error: 21.754, c_model: 11.321, actorQ: 4.664, a_model: 0.581
steps: 147475, episodes: 5900, mean episode reward: -1431.722, reward: -36.671, time: 6.489, TD error: 21.674, c_model: 11.28, actorQ: 4.715, a_model: 0.576
steps: 149975, episodes: 6000, mean episode reward: -1493.264, reward: -37.523, time: 6.546, TD error: 21.6, c_model: 11.241, actorQ: 4.767, a_model: 0.571
steps: 152475, episodes: 6100, mean episode reward: -1257.862, reward: -29.845, time: 6.94, TD error: 21.524, c_model: 11.201, actorQ: 4.831, a_model: 0.565
steps: 154975, episodes: 6200, mean episode reward: -1433.352, reward: -36.183, time: 6.912, TD error: 21.452, c_model: 11.163, actorQ: 4.882, a_model: 0.56
steps: 157475, episodes: 6300, mean episode reward: -1139.23, reward: -24.62, time: 6.663, TD error: 21.382, c_model: 11.126, actorQ: 4.933, a_model: 0.555
steps: 159975, episodes: 6400, mean episode reward: -1231.482, reward: -27.785, time: 6.648, TD error: 21.32, c_model: 11.092, actorQ: 4.984, a_model: 0.55
steps: 162475, episodes: 6500, mean episode reward: -1444.729, reward: -35.367, time: 6.936, TD error: 21.25, c_model: 11.055, actorQ: 5.035, a_model: 0.546
steps: 164975, episodes: 6600, mean episode reward: -1190.804, reward: -26.804, time: 7.078, TD error: 21.186, c_model: 11.021, actorQ: 5.086, a_model: 0.541
steps: 167475, episodes: 6700, mean episode reward: -1460.786, reward: -37.007, time: 7.819, TD error: 21.109, c_model: 10.979, actorQ: 5.15, a_model: 0.535
steps: 169975, episodes: 6800, mean episode reward: -1413.158, reward: -36.141, time: 6.585, TD error: 21.032, c_model: 10.937, actorQ: 5.201, a_model: 0.531
steps: 172475, episodes: 6900, mean episode reward: -1365.806, reward: -33.231, time: 6.696, TD error: 20.977, c_model: 10.907, actorQ: 5.252, a_model: 0.527
steps: 174975, episodes: 7000, mean episode reward: -1296.826, reward: -30.68, time: 6.595, TD error: 20.908, c_model: 10.87, actorQ: 5.302, a_model: 0.522
steps: 177475, episodes: 7100, mean episode reward: -1145.981, reward: -25.772, time: 6.963, TD error: 20.833, c_model: 10.829, actorQ: 5.353, a_model: 0.518
steps: 179975, episodes: 7200, mean episode reward: -1396.346, reward: -34.029, time: 6.884, TD error: 20.764, c_model: 10.791, actorQ: 5.404, a_model: 0.514
steps: 182475, episodes: 7300, mean episode reward: -1334.339, reward: -32.767, time: 6.641, TD error: 20.676, c_model: 10.743, actorQ: 5.467, a_model: 0.509
steps: 184975, episodes: 7400, mean episode reward: -1367.352, reward: -33.909, time: 6.791, TD error: 20.616, c_model: 10.71, actorQ: 5.518, a_model: 0.505
steps: 187475, episodes: 7500, mean episode reward: -1388.334, reward: -34.759, time: 6.82, TD error: 20.556, c_model: 10.676, actorQ: 5.569, a_model: 0.501
steps: 189975, episodes: 7600, mean episode reward: -1393.397, reward: -35.148, time: 6.857, TD error: 20.505, c_model: 10.647, actorQ: 5.62, a_model: 0.497
steps: 192475, episodes: 7700, mean episode reward: -1273.927, reward: -32.01, time: 6.637, TD error: 20.451, c_model: 10.616, actorQ: 5.671, a_model: 0.493
steps: 194975, episodes: 7800, mean episode reward: -1317.985, reward: -33.401, time: 6.849, TD error: 20.39, c_model: 10.582, actorQ: 5.721, a_model: 0.489
steps: 197475, episodes: 7900, mean episode reward: -1453.492, reward: -36.764, time: 6.752, TD error: 20.317, c_model: 10.54, actorQ: 5.785, a_model: 0.485
steps: 199975, episodes: 8000, mean episode reward: -1282.342, reward: -31.36, time: 6.601, TD error: 20.256, c_model: 10.506, actorQ: 5.836, a_model: 0.481
steps: 202475, episodes: 8100, mean episode reward: -1384.625, reward: -34.344, time: 6.799, TD error: 20.193, c_model: 10.47, actorQ: 5.887, a_model: 0.477
steps: 204975, episodes: 8200, mean episode reward: -1302.586, reward: -29.602, time: 6.832, TD error: 20.134, c_model: 10.437, actorQ: 5.938, a_model: 0.474
steps: 207475, episodes: 8300, mean episode reward: -1219.602, reward: -28.335, time: 6.719, TD error: 20.082, c_model: 10.407, actorQ: 5.989, a_model: 0.47
steps: 209975, episodes: 8400, mean episode reward: -1280.286, reward: -30.595, time: 6.614, TD error: 20.027, c_model: 10.375, actorQ: 6.04, a_model: 0.467
steps: 212475, episodes: 8500, mean episode reward: -1428.588, reward: -35.421, time: 7.052, TD error: 19.959, c_model: 10.336, actorQ: 6.103, a_model: 0.463
steps: 214975, episodes: 8600, mean episode reward: -1358.622, reward: -33.872, time: 6.745, TD error: 19.915, c_model: 10.31, actorQ: 6.155, a_model: 0.46
steps: 217475, episodes: 8700, mean episode reward: -1393.102, reward: -34.888, time: 7.42, TD error: 19.87, c_model: 10.283, actorQ: 6.206, a_model: 0.457
steps: 219975, episodes: 8800, mean episode reward: -1210.232, reward: -28.741, time: 7.207, TD error: 19.817, c_model: 10.252, actorQ: 6.257, a_model: 0.453
steps: 222475, episodes: 8900, mean episode reward: -1310.901, reward: -31.966, time: 6.635, TD error: 19.772, c_model: 10.226, actorQ: 6.308, a_model: 0.45
steps: 224975, episodes: 9000, mean episode reward: -1291.676, reward: -30.517, time: 6.703, TD error: 19.725, c_model: 10.198, actorQ: 6.36, a_model: 0.447
steps: 227475, episodes: 9100, mean episode reward: -1438.415, reward: -36.861, time: 6.893, TD error: 19.667, c_model: 10.164, actorQ: 6.424, a_model: 0.443
steps: 229975, episodes: 9200, mean episode reward: -1392.437, reward: -34.271, time: 6.627, TD error: 19.618, c_model: 10.135, actorQ: 6.476, a_model: 0.441
steps: 232475, episodes: 9300, mean episode reward: -1303.631, reward: -31.684, time: 6.687, TD error: 19.566, c_model: 10.105, actorQ: 6.528, a_model: 0.438
steps: 234975, episodes: 9400, mean episode reward: -1298.526, reward: -31.298, time: 6.723, TD error: 19.511, c_model: 10.073, actorQ: 6.579, a_model: 0.435
steps: 237475, episodes: 9500, mean episode reward: -1330.545, reward: -31.998, time: 6.686, TD error: 19.466, c_model: 10.047, actorQ: 6.631, a_model: 0.432
steps: 239975, episodes: 9600, mean episode reward: -1382.675, reward: -34.668, time: 7.079, TD error: 19.428, c_model: 10.024, actorQ: 6.683, a_model: 0.429
steps: 242475, episodes: 9700, mean episode reward: -1325.023, reward: -32.746, time: 6.75, TD error: 19.369, c_model: 9.99, actorQ: 6.748, a_model: 0.426
steps: 244975, episodes: 9800, mean episode reward: -1234.009, reward: -28.462, time: 6.625, TD error: 19.328, c_model: 9.965, actorQ: 6.801, a_model: 0.423
steps: 247475, episodes: 9900, mean episode reward: -1275.634, reward: -30.703, time: 6.732, TD error: 19.287, c_model: 9.941, actorQ: 6.853, a_model: 0.421
steps: 249975, episodes: 10000, mean episode reward: -1415.725, reward: -34.228, time: 6.73, TD error: 19.25, c_model: 9.919, actorQ: 6.905, a_model: 0.418
steps: 252475, episodes: 10100, mean episode reward: -1517.212, reward: -38.515, time: 6.681, TD error: 19.207, c_model: 9.893, actorQ: 6.958, a_model: 0.416
steps: 254975, episodes: 10200, mean episode reward: -1427.38, reward: -35.202, time: 6.65, TD error: 19.168, c_model: 9.87, actorQ: 7.01, a_model: 0.413
steps: 257475, episodes: 10300, mean episode reward: -1471.302, reward: -37.779, time: 6.668, TD error: 19.12, c_model: 9.842, actorQ: 7.076, a_model: 0.41
steps: 259975, episodes: 10400, mean episode reward: -1428.072, reward: -35.228, time: 6.511, TD error: 19.078, c_model: 9.818, actorQ: 7.129, a_model: 0.407
steps: 262475, episodes: 10500, mean episode reward: -1498.454, reward: -38.223, time: 6.577, TD error: 19.036, c_model: 9.793, actorQ: 7.181, a_model: 0.405
steps: 264975, episodes: 10600, mean episode reward: -1484.645, reward: -37.861, time: 6.787, TD error: 18.998, c_model: 9.771, actorQ: 7.234, a_model: 0.403
steps: 267475, episodes: 10700, mean episode reward: -1476.136, reward: -37.828, time: 6.604, TD error: 18.963, c_model: 9.75, actorQ: 7.287, a_model: 0.4
steps: 269975, episodes: 10800, mean episode reward: -1367.495, reward: -32.903, time: 6.789, TD error: 18.928, c_model: 9.73, actorQ: 7.34, a_model: 0.398
steps: 272475, episodes: 10900, mean episode reward: -1487.633, reward: -38.554, time: 6.898, TD error: 18.88, c_model: 9.702, actorQ: 7.406, a_model: 0.395
steps: 274975, episodes: 11000, mean episode reward: -1481.679, reward: -38.474, time: 6.763, TD error: 18.845, c_model: 9.682, actorQ: 7.459, a_model: 0.393
steps: 277475, episodes: 11100, mean episode reward: -1519.078, reward: -38.917, time: 6.597, TD error: 18.811, c_model: 9.661, actorQ: 7.511, a_model: 0.391
steps: 279975, episodes: 11200, mean episode reward: -1473.094, reward: -38.279, time: 7.53, TD error: 18.772, c_model: 9.639, actorQ: 7.563, a_model: 0.389
steps: 282475, episodes: 11300, mean episode reward: -1488.212, reward: -38.53, time: 7.483, TD error: 18.737, c_model: 9.619, actorQ: 7.614, a_model: 0.387
steps: 284975, episodes: 11400, mean episode reward: -1475.144, reward: -38.286, time: 6.721, TD error: 18.704, c_model: 9.6, actorQ: 7.665, a_model: 0.385
steps: 287475, episodes: 11500, mean episode reward: -1503.203, reward: -38.773, time: 6.771, TD error: 18.66, c_model: 9.575, actorQ: 7.726, a_model: 0.382
steps: 289975, episodes: 11600, mean episode reward: -1485.941, reward: -38.388, time: 6.555, TD error: 18.622, c_model: 9.553, actorQ: 7.773, a_model: 0.38
steps: 292475, episodes: 11700, mean episode reward: -1476.559, reward: -38.239, time: 6.595, TD error: 18.589, c_model: 9.534, actorQ: 7.822, a_model: 0.378
steps: 294975, episodes: 11800, mean episode reward: -1131.702, reward: -23.383, time: 6.699, TD error: 18.561, c_model: 9.517, actorQ: 7.868, a_model: 0.376
steps: 297475, episodes: 11900, mean episode reward: -706.118, reward: -10.454, time: 6.696, TD error: 18.52, c_model: 9.494, actorQ: 7.914, a_model: 0.374
steps: 299975, episodes: 12000, mean episode reward: -1059.575, reward: -19.537, time: 6.686, TD error: 18.483, c_model: 9.473, actorQ: 7.959, a_model: 0.372
steps: 302475, episodes: 12100, mean episode reward: -822.492, reward: -13.286, time: 6.694, TD error: 18.437, c_model: 9.447, actorQ: 8.016, a_model: 0.37
steps: 304975, episodes: 12200, mean episode reward: -719.811, reward: -10.532, time: 6.651, TD error: 18.395, c_model: 9.423, actorQ: 8.061, a_model: 0.368
steps: 307475, episodes: 12300, mean episode reward: -644.585, reward: -8.801, time: 6.63, TD error: 18.353, c_model: 9.4, actorQ: 8.107, a_model: 0.366
steps: 309975, episodes: 12400, mean episode reward: -653.629, reward: -8.791, time: 6.772, TD error: 18.315, c_model: 9.378, actorQ: 8.153, a_model: 0.364
steps: 312475, episodes: 12500, mean episode reward: -634.904, reward: -8.611, time: 6.536, TD error: 18.276, c_model: 9.357, actorQ: 8.198, a_model: 0.362
steps: 314975, episodes: 12600, mean episode reward: -635.8, reward: -8.573, time: 6.559, TD error: 18.238, c_model: 9.335, actorQ: 8.243, a_model: 0.36
steps: 317475, episodes: 12700, mean episode reward: -650.787, reward: -8.761, time: 6.773, TD error: 18.187, c_model: 9.307, actorQ: 8.297, a_model: 0.358
steps: 319975, episodes: 12800, mean episode reward: -668.041, reward: -8.974, time: 6.632, TD error: 18.143, c_model: 9.283, actorQ: 8.34, a_model: 0.356
steps: 322475, episodes: 12900, mean episode reward: -637.983, reward: -8.556, time: 6.551, TD error: 18.104, c_model: 9.262, actorQ: 8.38, a_model: 0.354
steps: 324975, episodes: 13000, mean episode reward: -636.151, reward: -8.575, time: 6.632, TD error: 18.067, c_model: 9.24, actorQ: 8.419, a_model: 0.353
steps: 327475, episodes: 13100, mean episode reward: -633.47, reward: -8.555, time: 6.635, TD error: 18.028, c_model: 9.219, actorQ: 8.459, a_model: 0.351
steps: 329975, episodes: 13200, mean episode reward: -623.6, reward: -8.415, time: 6.8, TD error: 17.985, c_model: 9.195, actorQ: 8.496, a_model: 0.349
steps: 332475, episodes: 13300, mean episode reward: -660.446, reward: -8.88, time: 7.108, TD error: 17.929, c_model: 9.164, actorQ: 8.543, a_model: 0.347
steps: 334975, episodes: 13400, mean episode reward: -660.345, reward: -8.873, time: 7.2, TD error: 17.888, c_model: 9.141, actorQ: 8.581, a_model: 0.346
steps: 337475, episodes: 13500, mean episode reward: -653.337, reward: -8.786, time: 6.644, TD error: 17.841, c_model: 9.116, actorQ: 8.619, a_model: 0.344
steps: 339975, episodes: 13600, mean episode reward: -644.822, reward: -8.667, time: 6.55, TD error: 17.797, c_model: 9.091, actorQ: 8.654, a_model: 0.342
steps: 342475, episodes: 13700, mean episode reward: -652.878, reward: -8.772, time: 6.517, TD error: 17.752, c_model: 9.067, actorQ: 8.69, a_model: 0.341
steps: 344975, episodes: 13800, mean episode reward: -671.227, reward: -9.043, time: 6.585, TD error: 17.705, c_model: 9.041, actorQ: 8.727, a_model: 0.339
steps: 347475, episodes: 13900, mean episode reward: -632.399, reward: -8.499, time: 6.817, TD error: 17.646, c_model: 9.009, actorQ: 8.77, a_model: 0.337
steps: 349975, episodes: 14000, mean episode reward: -650.478, reward: -8.759, time: 6.717, TD error: 17.605, c_model: 8.987, actorQ: 8.808, a_model: 0.336
steps: 352475, episodes: 14100, mean episode reward: -611.386, reward: -8.239, time: 7.012, TD error: 17.562, c_model: 8.963, actorQ: 8.846, a_model: 0.334
steps: 354975, episodes: 14200, mean episode reward: -662.746, reward: -8.916, time: 7.541, TD error: 17.517, c_model: 8.939, actorQ: 8.883, a_model: 0.333
steps: 357475, episodes: 14300, mean episode reward: -654.593, reward: -8.811, time: 6.958, TD error: 17.469, c_model: 8.913, actorQ: 8.921, a_model: 0.331
steps: 359975, episodes: 14400, mean episode reward: -637.009, reward: -8.587, time: 6.832, TD error: 17.424, c_model: 8.889, actorQ: 8.956, a_model: 0.33
steps: 362475, episodes: 14500, mean episode reward: -665.31, reward: -8.975, time: 8.249, TD error: 17.366, c_model: 8.858, actorQ: 9.002, a_model: 0.328
steps: 364975, episodes: 14600, mean episode reward: -646.709, reward: -8.697, time: 9.101, TD error: 17.322, c_model: 8.834, actorQ: 9.038, a_model: 0.326
steps: 367475, episodes: 14700, mean episode reward: -652.055, reward: -8.771, time: 8.145, TD error: 17.277, c_model: 8.81, actorQ: 9.077, a_model: 0.325
steps: 369975, episodes: 14800, mean episode reward: -663.808, reward: -8.923, time: 6.99, TD error: 17.234, c_model: 8.787, actorQ: 9.114, a_model: 0.323
steps: 372475, episodes: 14900, mean episode reward: -636.899, reward: -8.58, time: 6.886, TD error: 17.187, c_model: 8.763, actorQ: 9.152, a_model: 0.322
steps: 374975, episodes: 15000, mean episode reward: -648.024, reward: -8.735, time: 6.738, TD error: 17.141, c_model: 8.739, actorQ: 9.189, a_model: 0.32
steps: 377475, episodes: 15100, mean episode reward: -648.672, reward: -8.702, time: 6.983, TD error: 17.083, c_model: 8.709, actorQ: 9.236, a_model: 0.319
steps: 379975, episodes: 15200, mean episode reward: -637.945, reward: -8.58, time: 6.904, TD error: 17.039, c_model: 8.686, actorQ: 9.274, a_model: 0.317
steps: 382475, episodes: 15300, mean episode reward: -639.78, reward: -8.608, time: 6.559, TD error: 16.993, c_model: 8.662, actorQ: 9.312, a_model: 0.316
steps: 384975, episodes: 15400, mean episode reward: -615.622, reward: -8.278, time: 6.69, TD error: 16.947, c_model: 8.639, actorQ: 9.348, a_model: 0.315
steps: 387475, episodes: 15500, mean episode reward: -644.412, reward: -8.679, time: 6.547, TD error: 16.901, c_model: 8.615, actorQ: 9.386, a_model: 0.313
steps: 389975, episodes: 15600, mean episode reward: -649.878, reward: -8.742, time: 6.767, TD error: 16.856, c_model: 8.592, actorQ: 9.421, a_model: 0.312
steps: 392475, episodes: 15700, mean episode reward: -656.477, reward: -8.814, time: 7.432, TD error: 16.798, c_model: 8.562, actorQ: 9.469, a_model: 0.31
steps: 394975, episodes: 15800, mean episode reward: -626.033, reward: -8.436, time: 7.149, TD error: 16.75, c_model: 8.537, actorQ: 9.506, a_model: 0.309
steps: 397475, episodes: 15900, mean episode reward: -647.885, reward: -8.696, time: 6.463, TD error: 16.704, c_model: 8.514, actorQ: 9.544, a_model: 0.308
steps: 399975, episodes: 16000, mean episode reward: -645.964, reward: -8.683, time: 6.745, TD error: 16.659, c_model: 8.49, actorQ: 9.579, a_model: 0.306
steps: 402475, episodes: 16100, mean episode reward: -620.823, reward: -8.353, time: 6.57, TD error: 16.612, c_model: 8.466, actorQ: 9.616, a_model: 0.305
steps: 404975, episodes: 16200, mean episode reward: -643.739, reward: -8.681, time: 6.435, TD error: 16.566, c_model: 8.443, actorQ: 9.652, a_model: 0.304
steps: 407475, episodes: 16300, mean episode reward: -653.091, reward: -8.795, time: 6.529, TD error: 16.512, c_model: 8.415, actorQ: 9.7, a_model: 0.302
steps: 409975, episodes: 16400, mean episode reward: -635.31, reward: -8.544, time: 6.529, TD error: 16.466, c_model: 8.391, actorQ: 9.735, a_model: 0.301
steps: 412475, episodes: 16500, mean episode reward: -648.06, reward: -8.728, time: 6.544, TD error: 16.422, c_model: 8.368, actorQ: 9.772, a_model: 0.3
steps: 414975, episodes: 16600, mean episode reward: -662.463, reward: -8.902, time: 6.521, TD error: 16.376, c_model: 8.344, actorQ: 9.807, a_model: 0.299
steps: 417475, episodes: 16700, mean episode reward: -673.089, reward: -9.063, time: 6.477, TD error: 16.329, c_model: 8.321, actorQ: 9.844, a_model: 0.297
steps: 419975, episodes: 16800, mean episode reward: -649.636, reward: -8.774, time: 6.536, TD error: 16.282, c_model: 8.296, actorQ: 9.88, a_model: 0.296
steps: 422475, episodes: 16900, mean episode reward: -666.884, reward: -8.971, time: 6.634, TD error: 16.23, c_model: 8.269, actorQ: 9.927, a_model: 0.295
steps: 424975, episodes: 17000, mean episode reward: -643.816, reward: -8.653, time: 6.555, TD error: 16.187, c_model: 8.248, actorQ: 9.963, a_model: 0.294
steps: 427475, episodes: 17100, mean episode reward: -651.006, reward: -8.778, time: 6.535, TD error: 16.144, c_model: 8.226, actorQ: 10.001, a_model: 0.292
steps: 429975, episodes: 17200, mean episode reward: -646.4, reward: -8.711, time: 6.565, TD error: 16.1, c_model: 8.203, actorQ: 10.037, a_model: 0.291
steps: 432475, episodes: 17300, mean episode reward: -629.182, reward: -8.497, time: 6.538, TD error: 16.053, c_model: 8.18, actorQ: 10.073, a_model: 0.29
steps: 434975, episodes: 17400, mean episode reward: -645.95, reward: -8.676, time: 6.524, TD error: 16.01, c_model: 8.158, actorQ: 10.108, a_model: 0.289
steps: 437475, episodes: 17500, mean episode reward: -622.545, reward: -8.369, time: 6.64, TD error: 15.955, c_model: 8.13, actorQ: 10.154, a_model: 0.288
steps: 439975, episodes: 17600, mean episode reward: -632.448, reward: -8.514, time: 6.564, TD error: 15.908, c_model: 8.107, actorQ: 10.186, a_model: 0.286
steps: 442475, episodes: 17700, mean episode reward: -653.143, reward: -8.801, time: 6.678, TD error: 15.862, c_model: 8.083, actorQ: 10.221, a_model: 0.285
steps: 444975, episodes: 17800, mean episode reward: -632.877, reward: -8.488, time: 6.772, TD error: 15.82, c_model: 8.062, actorQ: 10.256, a_model: 0.284
steps: 447475, episodes: 17900, mean episode reward: -645.554, reward: -8.707, time: 6.646, TD error: 15.777, c_model: 8.04, actorQ: 10.291, a_model: 0.283
steps: 449975, episodes: 18000, mean episode reward: -633.578, reward: -8.587, time: 6.555, TD error: 15.734, c_model: 8.018, actorQ: 10.327, a_model: 0.282
steps: 452475, episodes: 18100, mean episode reward: -618.135, reward: -8.313, time: 6.563, TD error: 15.679, c_model: 7.991, actorQ: 10.37, a_model: 0.281
steps: 454975, episodes: 18200, mean episode reward: -647.801, reward: -8.724, time: 6.415, TD error: 15.636, c_model: 7.969, actorQ: 10.407, a_model: 0.28
steps: 457475, episodes: 18300, mean episode reward: -669.208, reward: -9.021, time: 6.558, TD error: 15.595, c_model: 7.948, actorQ: 10.443, a_model: 0.279
steps: 459975, episodes: 18400, mean episode reward: -653.004, reward: -8.807, time: 6.471, TD error: 15.549, c_model: 7.925, actorQ: 10.477, a_model: 0.278
steps: 462475, episodes: 18500, mean episode reward: -627.692, reward: -8.478, time: 6.537, TD error: 15.505, c_model: 7.902, actorQ: 10.512, a_model: 0.277
steps: 464975, episodes: 18600, mean episode reward: -651.466, reward: -8.76, time: 6.596, TD error: 15.464, c_model: 7.882, actorQ: 10.546, a_model: 0.276
steps: 467475, episodes: 18700, mean episode reward: -640.493, reward: -8.592, time: 6.548, TD error: 15.408, c_model: 7.853, actorQ: 10.589, a_model: 0.274
steps: 469975, episodes: 18800, mean episode reward: -663.36, reward: -8.926, time: 6.47, TD error: 15.366, c_model: 7.832, actorQ: 10.622, a_model: 0.273
steps: 472475, episodes: 18900, mean episode reward: -635.499, reward: -8.562, time: 6.528, TD error: 15.325, c_model: 7.811, actorQ: 10.654, a_model: 0.273
steps: 474975, episodes: 19000, mean episode reward: -653.034, reward: -8.795, time: 6.851, TD error: 15.284, c_model: 7.79, actorQ: 10.69, a_model: 0.272
steps: 477475, episodes: 19100, mean episode reward: -666.439, reward: -8.94, time: 6.549, TD error: 15.242, c_model: 7.769, actorQ: 10.722, a_model: 0.271
steps: 479975, episodes: 19200, mean episode reward: -632.716, reward: -8.54, time: 6.489, TD error: 15.202, c_model: 7.748, actorQ: 10.756, a_model: 0.27
steps: 482475, episodes: 19300, mean episode reward: -621.615, reward: -8.397, time: 6.711, TD error: 15.15, c_model: 7.722, actorQ: 10.798, a_model: 0.268
steps: 484975, episodes: 19400, mean episode reward: -641.007, reward: -8.679, time: 6.796, TD error: 15.111, c_model: 7.702, actorQ: 10.832, a_model: 0.267
steps: 487475, episodes: 19500, mean episode reward: -617.868, reward: -8.31, time: 6.684, TD error: 15.072, c_model: 7.683, actorQ: 10.866, a_model: 0.267
steps: 489975, episodes: 19600, mean episode reward: -617.078, reward: -8.309, time: 6.715, TD error: 15.031, c_model: 7.662, actorQ: 10.9, a_model: 0.266
steps: 492475, episodes: 19700, mean episode reward: -644.917, reward: -8.687, time: 6.666, TD error: 14.991, c_model: 7.642, actorQ: 10.933, a_model: 0.265
steps: 494975, episodes: 19800, mean episode reward: -612.661, reward: -8.278, time: 6.853, TD error: 14.951, c_model: 7.621, actorQ: 10.967, a_model: 0.264
steps: 497475, episodes: 19900, mean episode reward: -643.143, reward: -8.635, time: 6.731, TD error: 14.9, c_model: 7.596, actorQ: 11.01, a_model: 0.263
steps: 499975, episodes: 20000, mean episode reward: -650.74, reward: -8.784, time: 6.628, TD error: 14.86, c_model: 7.576, actorQ: 11.043, a_model: 0.262
steps: 502475, episodes: 20100, mean episode reward: -619.751, reward: -8.315, time: 6.531, TD error: 14.819, c_model: 7.555, actorQ: 11.076, a_model: 0.261
steps: 504975, episodes: 20200, mean episode reward: -634.502, reward: -8.528, time: 6.928, TD error: 14.781, c_model: 7.536, actorQ: 11.108, a_model: 0.26
steps: 507475, episodes: 20300, mean episode reward: -641.405, reward: -8.649, time: 7.127, TD error: 14.741, c_model: 7.516, actorQ: 11.141, a_model: 0.259
steps: 509975, episodes: 20400, mean episode reward: -637.451, reward: -8.568, time: 7.772, TD error: 14.701, c_model: 7.496, actorQ: 11.172, a_model: 0.258
steps: 512475, episodes: 20500, mean episode reward: -628.037, reward: -8.434, time: 6.983, TD error: 14.654, c_model: 7.472, actorQ: 11.215, a_model: 0.257
steps: 514975, episodes: 20600, mean episode reward: -645.304, reward: -8.707, time: 6.563, TD error: 14.615, c_model: 7.453, actorQ: 11.247, a_model: 0.256
steps: 517475, episodes: 20700, mean episode reward: -642.328, reward: -8.653, time: 6.612, TD error: 14.576, c_model: 7.433, actorQ: 11.28, a_model: 0.256
steps: 519975, episodes: 20800, mean episode reward: -653.13, reward: -8.787, time: 6.621, TD error: 14.537, c_model: 7.413, actorQ: 11.311, a_model: 0.255
steps: 522475, episodes: 20900, mean episode reward: -661.459, reward: -8.907, time: 6.501, TD error: 14.498, c_model: 7.393, actorQ: 11.343, a_model: 0.254
steps: 524975, episodes: 21000, mean episode reward: -622.838, reward: -8.383, time: 6.58, TD error: 14.461, c_model: 7.374, actorQ: 11.377, a_model: 0.253
steps: 527475, episodes: 21100, mean episode reward: -662.393, reward: -8.861, time: 6.529, TD error: 14.414, c_model: 7.351, actorQ: 11.419, a_model: 0.252
steps: 529975, episodes: 21200, mean episode reward: -628.803, reward: -8.449, time: 6.458, TD error: 14.375, c_model: 7.331, actorQ: 11.45, a_model: 0.251
steps: 532475, episodes: 21300, mean episode reward: -626.9, reward: -8.473, time: 6.513, TD error: 14.337, c_model: 7.312, actorQ: 11.483, a_model: 0.25
steps: 534975, episodes: 21400, mean episode reward: -639.138, reward: -8.585, time: 6.418, TD error: 14.301, c_model: 7.294, actorQ: 11.514, a_model: 0.25
steps: 537475, episodes: 21500, mean episode reward: -647.752, reward: -8.726, time: 6.485, TD error: 14.264, c_model: 7.275, actorQ: 11.546, a_model: 0.249
steps: 539975, episodes: 21600, mean episode reward: -649.457, reward: -8.733, time: 6.478, TD error: 14.227, c_model: 7.257, actorQ: 11.577, a_model: 0.248
steps: 542475, episodes: 21700, mean episode reward: -631.126, reward: -8.486, time: 6.614, TD error: 14.182, c_model: 7.234, actorQ: 11.617, a_model: 0.247
steps: 544975, episodes: 21800, mean episode reward: -650.309, reward: -8.756, time: 6.491, TD error: 14.147, c_model: 7.216, actorQ: 11.649, a_model: 0.246
steps: 547475, episodes: 21900, mean episode reward: -639.434, reward: -8.583, time: 6.646, TD error: 14.111, c_model: 7.198, actorQ: 11.679, a_model: 0.246
steps: 549975, episodes: 22000, mean episode reward: -643.079, reward: -8.629, time: 6.614, TD error: 14.076, c_model: 7.18, actorQ: 11.712, a_model: 0.245
steps: 552475, episodes: 22100, mean episode reward: -645.207, reward: -8.697, time: 6.636, TD error: 14.038, c_model: 7.161, actorQ: 11.742, a_model: 0.244
steps: 554975, episodes: 22200, mean episode reward: -632.291, reward: -8.478, time: 6.564, TD error: 14.003, c_model: 7.143, actorQ: 11.773, a_model: 0.244
steps: 557475, episodes: 22300, mean episode reward: -658.215, reward: -8.87, time: 6.871, TD error: 13.959, c_model: 7.122, actorQ: 11.814, a_model: 0.243
steps: 559975, episodes: 22400, mean episode reward: -613.546, reward: -8.26, time: 6.737, TD error: 13.924, c_model: 7.104, actorQ: 11.842, a_model: 0.242
steps: 562475, episodes: 22500, mean episode reward: -639.388, reward: -8.571, time: 6.71, TD error: 13.888, c_model: 7.086, actorQ: 11.872, a_model: 0.241
steps: 564975, episodes: 22600, mean episode reward: -613.544, reward: -8.217, time: 6.551, TD error: 13.853, c_model: 7.069, actorQ: 11.902, a_model: 0.24
steps: 567475, episodes: 22700, mean episode reward: -636.116, reward: -8.518, time: 6.635, TD error: 13.819, c_model: 7.052, actorQ: 11.933, a_model: 0.24
steps: 569975, episodes: 22800, mean episode reward: -649.046, reward: -8.694, time: 6.814, TD error: 13.785, c_model: 7.035, actorQ: 11.965, a_model: 0.239
steps: 572475, episodes: 22900, mean episode reward: -637.558, reward: -8.545, time: 6.48, TD error: 13.742, c_model: 7.014, actorQ: 12.003, a_model: 0.238
steps: 574975, episodes: 23000, mean episode reward: -662.221, reward: -8.907, time: 6.433, TD error: 13.708, c_model: 6.997, actorQ: 12.033, a_model: 0.238
steps: 577475, episodes: 23100, mean episode reward: -669.405, reward: -8.973, time: 6.402, TD error: 13.674, c_model: 6.98, actorQ: 12.064, a_model: 0.237
steps: 579975, episodes: 23200, mean episode reward: -660.352, reward: -8.852, time: 6.494, TD error: 13.639, c_model: 6.962, actorQ: 12.092, a_model: 0.236
steps: 582475, episodes: 23300, mean episode reward: -623.617, reward: -8.351, time: 6.542, TD error: 13.606, c_model: 6.946, actorQ: 12.123, a_model: 0.235
steps: 584975, episodes: 23400, mean episode reward: -610.3, reward: -8.153, time: 6.588, TD error: 13.573, c_model: 6.93, actorQ: 12.152, a_model: 0.235
steps: 587475, episodes: 23500, mean episode reward: -614.409, reward: -8.229, time: 6.631, TD error: 13.531, c_model: 6.909, actorQ: 12.191, a_model: 0.234
steps: 589975, episodes: 23600, mean episode reward: -629.571, reward: -8.424, time: 6.503, TD error: 13.496, c_model: 6.892, actorQ: 12.22, a_model: 0.233
steps: 592475, episodes: 23700, mean episode reward: -622.887, reward: -8.356, time: 6.659, TD error: 13.464, c_model: 6.875, actorQ: 12.251, a_model: 0.233
steps: 594975, episodes: 23800, mean episode reward: -644.584, reward: -8.617, time: 6.567, TD error: 13.431, c_model: 6.859, actorQ: 12.28, a_model: 0.232
steps: 597475, episodes: 23900, mean episode reward: -637.706, reward: -8.539, time: 6.59, TD error: 13.396, c_model: 6.841, actorQ: 12.308, a_model: 0.231
steps: 599975, episodes: 24000, mean episode reward: -633.298, reward: -8.442, time: 6.524, TD error: 13.363, c_model: 6.825, actorQ: 12.338, a_model: 0.231
steps: 602475, episodes: 24100, mean episode reward: -602.214, reward: -8.043, time: 6.667, TD error: 13.249, c_model: 6.768, actorQ: 12.409, a_model: 0.227
steps: 604975, episodes: 24200, mean episode reward: -646.422, reward: -8.578, time: 6.625, TD error: 13.127, c_model: 6.706, actorQ: 12.484, a_model: 0.222
steps: 607475, episodes: 24300, mean episode reward: -616.88, reward: -8.26, time: 6.784, TD error: 13.014, c_model: 6.649, actorQ: 12.56, a_model: 0.218
steps: 609975, episodes: 24400, mean episode reward: -634.174, reward: -8.508, time: 6.603, TD error: 12.901, c_model: 6.593, actorQ: 12.633, a_model: 0.214
steps: 612475, episodes: 24500, mean episode reward: -622.774, reward: -8.34, time: 6.528, TD error: 12.792, c_model: 6.538, actorQ: 12.707, a_model: 0.21
steps: 614975, episodes: 24600, mean episode reward: -640.793, reward: -8.581, time: 6.609, TD error: 12.685, c_model: 6.484, actorQ: 12.778, a_model: 0.206
steps: 617475, episodes: 24700, mean episode reward: -625.772, reward: -8.369, time: 6.807, TD error: 12.554, c_model: 6.417, actorQ: 12.868, a_model: 0.2
steps: 619975, episodes: 24800, mean episode reward: -626.497, reward: -8.343, time: 7.011, TD error: 12.454, c_model: 6.366, actorQ: 12.937, a_model: 0.196
steps: 622475, episodes: 24900, mean episode reward: -645.458, reward: -8.64, time: 6.728, TD error: 12.357, c_model: 6.317, actorQ: 13.005, a_model: 0.192
steps: 624975, episodes: 25000, mean episode reward: -631.127, reward: -8.401, time: 6.956, TD error: 12.268, c_model: 6.271, actorQ: 13.075, a_model: 0.189
steps: 627475, episodes: 25100, mean episode reward: -636.818, reward: -8.498, time: 7.586, TD error: 12.189, c_model: 6.23, actorQ: 13.144, a_model: 0.186
steps: 629975, episodes: 25200, mean episode reward: -621.18, reward: -8.263, time: 6.652, TD error: 12.112, c_model: 6.19, actorQ: 13.213, a_model: 0.183
steps: 632475, episodes: 25300, mean episode reward: -650.458, reward: -8.666, time: 6.716, TD error: 12.021, c_model: 6.143, actorQ: 13.3, a_model: 0.179
steps: 634975, episodes: 25400, mean episode reward: -627.845, reward: -8.404, time: 6.823, TD error: 11.952, c_model: 6.107, actorQ: 13.368, a_model: 0.176
steps: 637475, episodes: 25500, mean episode reward: -614.723, reward: -8.228, time: 6.773, TD error: 11.881, c_model: 6.07, actorQ: 13.438, a_model: 0.174
steps: 639975, episodes: 25600, mean episode reward: -612.386, reward: -8.094, time: 6.731, TD error: 11.812, c_model: 6.035, actorQ: 13.507, a_model: 0.171
steps: 642475, episodes: 25700, mean episode reward: -633.833, reward: -8.431, time: 6.897, TD error: 11.744, c_model: 5.999, actorQ: 13.576, a_model: 0.169
steps: 644975, episodes: 25800, mean episode reward: -633.89, reward: -8.451, time: 6.983, TD error: 11.676, c_model: 5.964, actorQ: 13.643, a_model: 0.166
steps: 647475, episodes: 25900, mean episode reward: -622.366, reward: -8.245, time: 6.731, TD error: 11.596, c_model: 5.922, actorQ: 13.727, a_model: 0.164
steps: 649975, episodes: 26000, mean episode reward: -605.541, reward: -8.072, time: 6.674, TD error: 11.533, c_model: 5.889, actorQ: 13.794, a_model: 0.161
steps: 652475, episodes: 26100, mean episode reward: -611.613, reward: -8.085, time: 6.557, TD error: 11.469, c_model: 5.856, actorQ: 13.863, a_model: 0.159
steps: 654975, episodes: 26200, mean episode reward: -616.502, reward: -8.238, time: 6.546, TD error: 11.407, c_model: 5.824, actorQ: 13.93, a_model: 0.158
steps: 657475, episodes: 26300, mean episode reward: -612.136, reward: -8.089, time: 6.527, TD error: 11.347, c_model: 5.792, actorQ: 13.998, a_model: 0.156
steps: 659975, episodes: 26400, mean episode reward: -618.564, reward: -8.211, time: 6.61, TD error: 11.287, c_model: 5.761, actorQ: 14.065, a_model: 0.154
steps: 662475, episodes: 26500, mean episode reward: -603.341, reward: -8.131, time: 6.641, TD error: 11.216, c_model: 5.723, actorQ: 14.149, a_model: 0.152
steps: 664975, episodes: 26600, mean episode reward: -603.406, reward: -8.024, time: 6.682, TD error: 11.16, c_model: 5.694, actorQ: 14.217, a_model: 0.15
steps: 667475, episodes: 26700, mean episode reward: -591.979, reward: -7.821, time: 6.86, TD error: 11.1, c_model: 5.663, actorQ: 14.282, a_model: 0.149
steps: 669975, episodes: 26800, mean episode reward: -587.027, reward: -7.742, time: 6.882, TD error: 11.042, c_model: 5.632, actorQ: 14.35, a_model: 0.147
steps: 672475, episodes: 26900, mean episode reward: -602.468, reward: -7.896, time: 6.968, TD error: 10.984, c_model: 5.602, actorQ: 14.415, a_model: 0.146
steps: 674975, episodes: 27000, mean episode reward: -575.969, reward: -7.62, time: 6.554, TD error: 10.926, c_model: 5.571, actorQ: 14.481, a_model: 0.144
steps: 677475, episodes: 27100, mean episode reward: -593.277, reward: -7.846, time: 6.949, TD error: 10.853, c_model: 5.533, actorQ: 14.562, a_model: 0.142
steps: 679975, episodes: 27200, mean episode reward: -605.328, reward: -7.785, time: 6.565, TD error: 10.792, c_model: 5.501, actorQ: 14.627, a_model: 0.141
steps: 682475, episodes: 27300, mean episode reward: -611.258, reward: -8.02, time: 6.655, TD error: 10.733, c_model: 5.47, actorQ: 14.693, a_model: 0.139
steps: 684975, episodes: 27400, mean episode reward: -595.406, reward: -7.878, time: 6.759, TD error: 10.676, c_model: 5.44, actorQ: 14.759, a_model: 0.138
steps: 687475, episodes: 27500, mean episode reward: -603.626, reward: -7.984, time: 6.603, TD error: 10.619, c_model: 5.41, actorQ: 14.823, a_model: 0.136
steps: 689975, episodes: 27600, mean episode reward: -622.504, reward: -8.215, time: 6.569, TD error: 10.56, c_model: 5.379, actorQ: 14.887, a_model: 0.135
steps: 692475, episodes: 27700, mean episode reward: -606.757, reward: -7.861, time: 6.629, TD error: 10.486, c_model: 5.34, actorQ: 14.967, a_model: 0.133
steps: 694975, episodes: 27800, mean episode reward: -599.762, reward: -7.776, time: 6.566, TD error: 10.427, c_model: 5.309, actorQ: 15.029, a_model: 0.131
steps: 697475, episodes: 27900, mean episode reward: -601.506, reward: -7.867, time: 6.596, TD error: 10.367, c_model: 5.277, actorQ: 15.093, a_model: 0.13
steps: 699975, episodes: 28000, mean episode reward: -592.731, reward: -7.672, time: 6.563, TD error: 10.308, c_model: 5.246, actorQ: 15.155, a_model: 0.128
steps: 702475, episodes: 28100, mean episode reward: -603.29, reward: -7.915, time: 6.539, TD error: 10.251, c_model: 5.215, actorQ: 15.217, a_model: 0.127
steps: 704975, episodes: 28200, mean episode reward: -597.474, reward: -7.858, time: 6.424, TD error: 10.196, c_model: 5.186, actorQ: 15.28, a_model: 0.126
steps: 707475, episodes: 28300, mean episode reward: -604.013, reward: -7.905, time: 6.56, TD error: 10.123, c_model: 5.148, actorQ: 15.357, a_model: 0.124
steps: 709975, episodes: 28400, mean episode reward: -616.67, reward: -8.096, time: 6.482, TD error: 10.065, c_model: 5.117, actorQ: 15.42, a_model: 0.123
steps: 712475, episodes: 28500, mean episode reward: -613.619, reward: -8.016, time: 6.451, TD error: 10.008, c_model: 5.087, actorQ: 15.481, a_model: 0.122
steps: 714975, episodes: 28600, mean episode reward: -628.014, reward: -8.177, time: 6.519, TD error: 9.949, c_model: 5.056, actorQ: 15.542, a_model: 0.121
steps: 717475, episodes: 28700, mean episode reward: -629.285, reward: -8.291, time: 6.489, TD error: 9.893, c_model: 5.026, actorQ: 15.602, a_model: 0.12
steps: 719975, episodes: 28800, mean episode reward: -589.634, reward: -7.672, time: 6.507, TD error: 9.839, c_model: 4.997, actorQ: 15.663, a_model: 0.119
steps: 722475, episodes: 28900, mean episode reward: -584.195, reward: -7.682, time: 6.554, TD error: 9.771, c_model: 4.961, actorQ: 15.738, a_model: 0.117
steps: 724975, episodes: 29000, mean episode reward: -608.39, reward: -7.712, time: 6.416, TD error: 9.715, c_model: 4.932, actorQ: 15.798, a_model: 0.116
steps: 727475, episodes: 29100, mean episode reward: -618.16, reward: -8.084, time: 6.57, TD error: 9.66, c_model: 4.903, actorQ: 15.858, a_model: 0.115
steps: 729975, episodes: 29200, mean episode reward: -605.574, reward: -7.949, time: 6.533, TD error: 9.608, c_model: 4.875, actorQ: 15.916, a_model: 0.114
steps: 732475, episodes: 29300, mean episode reward: -624.505, reward: -8.129, time: 6.531, TD error: 9.553, c_model: 4.846, actorQ: 15.976, a_model: 0.113
steps: 734975, episodes: 29400, mean episode reward: -636.541, reward: -8.214, time: 6.517, TD error: 9.503, c_model: 4.82, actorQ: 16.036, a_model: 0.112
steps: 737475, episodes: 29500, mean episode reward: -604.238, reward: -8.125, time: 6.609, TD error: 9.436, c_model: 4.784, actorQ: 16.111, a_model: 0.111
steps: 739975, episodes: 29600, mean episode reward: -591.792, reward: -7.978, time: 6.507, TD error: 9.385, c_model: 4.757, actorQ: 16.169, a_model: 0.111
steps: 742475, episodes: 29700, mean episode reward: -614.514, reward: -8.215, time: 7.525, TD error: 9.333, c_model: 4.73, actorQ: 16.229, a_model: 0.11
steps: 744975, episodes: 29800, mean episode reward: -610.521, reward: -8.179, time: 7.135, TD error: 9.28, c_model: 4.703, actorQ: 16.289, a_model: 0.109
steps: 747475, episodes: 29900, mean episode reward: -631.136, reward: -8.297, time: 6.53, TD error: 9.231, c_model: 4.677, actorQ: 16.349, a_model: 0.108
steps: 749975, episodes: 30000, mean episode reward: -616.163, reward: -8.107, time: 6.593, TD error: 9.181, c_model: 4.651, actorQ: 16.407, a_model: 0.107
steps: 752475, episodes: 30100, mean episode reward: -619.306, reward: -8.342, time: 6.774, TD error: 9.115, c_model: 4.616, actorQ: 16.481, a_model: 0.106
steps: 754975, episodes: 30200, mean episode reward: -618.252, reward: -8.234, time: 6.384, TD error: 9.066, c_model: 4.591, actorQ: 16.541, a_model: 0.106
steps: 757475, episodes: 30300, mean episode reward: -637.115, reward: -8.53, time: 6.753, TD error: 9.016, c_model: 4.565, actorQ: 16.597, a_model: 0.105
steps: 759975, episodes: 30400, mean episode reward: -633.969, reward: -8.463, time: 6.627, TD error: 8.965, c_model: 4.538, actorQ: 16.657, a_model: 0.104
steps: 762475, episodes: 30500, mean episode reward: -615.394, reward: -8.097, time: 6.57, TD error: 8.918, c_model: 4.514, actorQ: 16.715, a_model: 0.104
steps: 764975, episodes: 30600, mean episode reward: -603.765, reward: -7.903, time: 6.583, TD error: 8.867, c_model: 4.487, actorQ: 16.772, a_model: 0.103
steps: 767475, episodes: 30700, mean episode reward: -610.642, reward: -8.16, time: 6.563, TD error: 8.806, c_model: 4.455, actorQ: 16.843, a_model: 0.102
steps: 769975, episodes: 30800, mean episode reward: -611.011, reward: -8.087, time: 6.602, TD error: 8.76, c_model: 4.432, actorQ: 16.9, a_model: 0.102
steps: 772475, episodes: 30900, mean episode reward: -604.098, reward: -8.06, time: 6.51, TD error: 8.709, c_model: 4.405, actorQ: 16.956, a_model: 0.101
steps: 774975, episodes: 31000, mean episode reward: -605.565, reward: -8.105, time: 6.555, TD error: 8.663, c_model: 4.382, actorQ: 17.014, a_model: 0.1
steps: 777475, episodes: 31100, mean episode reward: -608.599, reward: -8.011, time: 6.525, TD error: 8.618, c_model: 4.359, actorQ: 17.071, a_model: 0.1
steps: 779975, episodes: 31200, mean episode reward: -618.641, reward: -8.128, time: 6.506, TD error: 8.573, c_model: 4.336, actorQ: 17.127, a_model: 0.099
steps: 782475, episodes: 31300, mean episode reward: -622.22, reward: -8.119, time: 7.306, TD error: 8.518, c_model: 4.307, actorQ: 17.197, a_model: 0.099
steps: 784975, episodes: 31400, mean episode reward: -621.616, reward: -8.122, time: 7.226, TD error: 8.472, c_model: 4.284, actorQ: 17.252, a_model: 0.098
steps: 787475, episodes: 31500, mean episode reward: -609.473, reward: -7.988, time: 6.409, TD error: 8.425, c_model: 4.26, actorQ: 17.306, a_model: 0.098
steps: 789975, episodes: 31600, mean episode reward: -628.473, reward: -8.14, time: 6.387, TD error: 8.377, c_model: 4.236, actorQ: 17.362, a_model: 0.097
steps: 792475, episodes: 31700, mean episode reward: -623.155, reward: -8.248, time: 6.35, TD error: 8.33, c_model: 4.212, actorQ: 17.417, a_model: 0.097
steps: 794975, episodes: 31800, mean episode reward: -625.041, reward: -8.317, time: 6.391, TD error: 8.286, c_model: 4.19, actorQ: 17.472, a_model: 0.096
steps: 797475, episodes: 31900, mean episode reward: -636.047, reward: -8.287, time: 6.92, TD error: 8.23, c_model: 4.162, actorQ: 17.54, a_model: 0.096
steps: 799975, episodes: 32000, mean episode reward: -623.732, reward: -8.405, time: 6.484, TD error: 8.188, c_model: 4.14, actorQ: 17.595, a_model: 0.095
steps: 802475, episodes: 32100, mean episode reward: -631.518, reward: -8.191, time: 6.368, TD error: 8.145, c_model: 4.119, actorQ: 17.651, a_model: 0.095
steps: 804975, episodes: 32200, mean episode reward: -626.329, reward: -8.378, time: 6.363, TD error: 8.102, c_model: 4.097, actorQ: 17.704, a_model: 0.094
steps: 807475, episodes: 32300, mean episode reward: -602.315, reward: -8.029, time: 6.941, TD error: 8.057, c_model: 4.075, actorQ: 17.759, a_model: 0.094
steps: 809975, episodes: 32400, mean episode reward: -609.606, reward: -8.064, time: 6.353, TD error: 8.014, c_model: 4.053, actorQ: 17.813, a_model: 0.093
steps: 812475, episodes: 32500, mean episode reward: -612.079, reward: -7.98, time: 6.57, TD error: 7.96, c_model: 4.026, actorQ: 17.881, a_model: 0.093
steps: 814975, episodes: 32600, mean episode reward: -605.031, reward: -7.967, time: 6.453, TD error: 7.913, c_model: 4.003, actorQ: 17.934, a_model: 0.093
steps: 817475, episodes: 32700, mean episode reward: -615.521, reward: -8.265, time: 6.366, TD error: 7.867, c_model: 3.98, actorQ: 17.988, a_model: 0.092
steps: 819975, episodes: 32800, mean episode reward: -611.963, reward: -8.199, time: 6.419, TD error: 7.824, c_model: 3.958, actorQ: 18.041, a_model: 0.092
steps: 822475, episodes: 32900, mean episode reward: -590.552, reward: -7.795, time: 6.422, TD error: 7.779, c_model: 3.936, actorQ: 18.094, a_model: 0.091
steps: 824975, episodes: 33000, mean episode reward: -590.824, reward: -7.767, time: 6.391, TD error: 7.733, c_model: 3.913, actorQ: 18.144, a_model: 0.091
steps: 827475, episodes: 33100, mean episode reward: -586.232, reward: -7.699, time: 6.52, TD error: 7.678, c_model: 3.886, actorQ: 18.211, a_model: 0.09
steps: 829975, episodes: 33200, mean episode reward: -622.502, reward: -8.203, time: 6.506, TD error: 7.634, c_model: 3.864, actorQ: 18.263, a_model: 0.09
steps: 832475, episodes: 33300, mean episode reward: -623.006, reward: -8.155, time: 6.63, TD error: 7.594, c_model: 3.844, actorQ: 18.313, a_model: 0.09
steps: 834975, episodes: 33400, mean episode reward: -589.718, reward: -7.772, time: 6.662, TD error: 7.555, c_model: 3.825, actorQ: 18.366, a_model: 0.089
steps: 837475, episodes: 33500, mean episode reward: -605.234, reward: -7.845, time: 6.449, TD error: 7.512, c_model: 3.804, actorQ: 18.417, a_model: 0.089
steps: 839975, episodes: 33600, mean episode reward: -604.088, reward: -7.939, time: 6.432, TD error: 7.467, c_model: 3.782, actorQ: 18.469, a_model: 0.089
steps: 842475, episodes: 33700, mean episode reward: -591.661, reward: -7.768, time: 6.497, TD error: 7.415, c_model: 3.756, actorQ: 18.533, a_model: 0.088
steps: 844975, episodes: 33800, mean episode reward: -597.928, reward: -7.763, time: 6.406, TD error: 7.371, c_model: 3.734, actorQ: 18.584, a_model: 0.088
steps: 847475, episodes: 33900, mean episode reward: -589.716, reward: -7.623, time: 6.387, TD error: 7.328, c_model: 3.713, actorQ: 18.635, a_model: 0.087
steps: 849975, episodes: 34000, mean episode reward: -597.818, reward: -8.008, time: 6.372, TD error: 7.282, c_model: 3.69, actorQ: 18.685, a_model: 0.087
steps: 852475, episodes: 34100, mean episode reward: -616.675, reward: -8.198, time: 6.375, TD error: 7.241, c_model: 3.67, actorQ: 18.735, a_model: 0.087
steps: 854975, episodes: 34200, mean episode reward: -599.846, reward: -7.871, time: 6.358, TD error: 7.199, c_model: 3.649, actorQ: 18.783, a_model: 0.086
steps: 857475, episodes: 34300, mean episode reward: -601.529, reward: -7.857, time: 6.509, TD error: 7.146, c_model: 3.623, actorQ: 18.846, a_model: 0.086
steps: 859975, episodes: 34400, mean episode reward: -589.25, reward: -7.669, time: 7.133, TD error: 7.104, c_model: 3.602, actorQ: 18.893, a_model: 0.086
steps: 862475, episodes: 34500, mean episode reward: -605.895, reward: -7.937, time: 7.133, TD error: 7.063, c_model: 3.582, actorQ: 18.943, a_model: 0.085
steps: 864975, episodes: 34600, mean episode reward: -605.573, reward: -7.925, time: 6.405, TD error: 7.021, c_model: 3.561, actorQ: 18.991, a_model: 0.085
steps: 867475, episodes: 34700, mean episode reward: -615.37, reward: -8.099, time: 6.338, TD error: 6.977, c_model: 3.539, actorQ: 19.039, a_model: 0.085
steps: 869975, episodes: 34800, mean episode reward: -595.42, reward: -7.859, time: 6.406, TD error: 6.934, c_model: 3.517, actorQ: 19.086, a_model: 0.084
steps: 872475, episodes: 34900, mean episode reward: -615.914, reward: -8.07, time: 6.426, TD error: 6.883, c_model: 3.492, actorQ: 19.145, a_model: 0.084
steps: 874975, episodes: 35000, mean episode reward: -588.741, reward: -7.703, time: 6.34, TD error: 6.839, c_model: 3.47, actorQ: 19.192, a_model: 0.084
steps: 877475, episodes: 35100, mean episode reward: -600.782, reward: -7.854, time: 6.323, TD error: 6.797, c_model: 3.449, actorQ: 19.239, a_model: 0.083
steps: 879975, episodes: 35200, mean episode reward: -601.275, reward: -7.84, time: 6.392, TD error: 6.756, c_model: 3.429, actorQ: 19.286, a_model: 0.083
steps: 882475, episodes: 35300, mean episode reward: -571.806, reward: -7.491, time: 6.385, TD error: 6.713, c_model: 3.407, actorQ: 19.332, a_model: 0.083
steps: 884975, episodes: 35400, mean episode reward: -610.754, reward: -8.035, time: 6.361, TD error: 6.672, c_model: 3.387, actorQ: 19.38, a_model: 0.083
steps: 887475, episodes: 35500, mean episode reward: -603.795, reward: -7.896, time: 6.499, TD error: 6.621, c_model: 3.361, actorQ: 19.439, a_model: 0.082
steps: 889975, episodes: 35600, mean episode reward: -586.818, reward: -7.614, time: 6.396, TD error: 6.582, c_model: 3.341, actorQ: 19.487, a_model: 0.082
steps: 892475, episodes: 35700, mean episode reward: -591.318, reward: -7.614, time: 6.373, TD error: 6.54, c_model: 3.321, actorQ: 19.535, a_model: 0.082
steps: 894975, episodes: 35800, mean episode reward: -587.039, reward: -7.814, time: 6.548, TD error: 6.496, c_model: 3.299, actorQ: 19.583, a_model: 0.082
steps: 897475, episodes: 35900, mean episode reward: -598.955, reward: -7.78, time: 6.48, TD error: 6.459, c_model: 3.281, actorQ: 19.631, a_model: 0.081
steps: 899975, episodes: 36000, mean episode reward: -615.505, reward: -7.992, time: 6.649, TD error: 6.421, c_model: 3.261, actorQ: 19.678, a_model: 0.081
steps: 902475, episodes: 36100, mean episode reward: -586.483, reward: -7.653, time: 6.544, TD error: 6.374, c_model: 3.238, actorQ: 19.739, a_model: 0.081
steps: 904975, episodes: 36200, mean episode reward: -593.142, reward: -7.532, time: 6.476, TD error: 6.34, c_model: 3.221, actorQ: 19.787, a_model: 0.081
steps: 907475, episodes: 36300, mean episode reward: -589.354, reward: -7.595, time: 6.368, TD error: 6.305, c_model: 3.204, actorQ: 19.835, a_model: 0.08
steps: 909975, episodes: 36400, mean episode reward: -600.818, reward: -7.891, time: 6.391, TD error: 6.269, c_model: 3.185, actorQ: 19.88, a_model: 0.08
steps: 912475, episodes: 36500, mean episode reward: -595.118, reward: -7.71, time: 6.412, TD error: 6.233, c_model: 3.167, actorQ: 19.928, a_model: 0.08
steps: 914975, episodes: 36600, mean episode reward: -594.601, reward: -7.793, time: 6.493, TD error: 6.198, c_model: 3.15, actorQ: 19.974, a_model: 0.08
steps: 917475, episodes: 36700, mean episode reward: -593.827, reward: -7.917, time: 6.557, TD error: 6.156, c_model: 3.128, actorQ: 20.033, a_model: 0.079
steps: 919975, episodes: 36800, mean episode reward: -585.295, reward: -7.608, time: 6.379, TD error: 6.123, c_model: 3.112, actorQ: 20.079, a_model: 0.079
steps: 922475, episodes: 36900, mean episode reward: -590.159, reward: -7.706, time: 6.369, TD error: 6.089, c_model: 3.095, actorQ: 20.127, a_model: 0.079
steps: 924975, episodes: 37000, mean episode reward: -578.705, reward: -7.492, time: 6.453, TD error: 6.053, c_model: 3.077, actorQ: 20.175, a_model: 0.079
steps: 927475, episodes: 37100, mean episode reward: -589.322, reward: -7.837, time: 6.297, TD error: 6.018, c_model: 3.059, actorQ: 20.224, a_model: 0.078
steps: 929975, episodes: 37200, mean episode reward: -606.085, reward: -7.72, time: 6.412, TD error: 5.987, c_model: 3.044, actorQ: 20.273, a_model: 0.078
steps: 932475, episodes: 37300, mean episode reward: -594.873, reward: -7.742, time: 6.434, TD error: 5.949, c_model: 3.025, actorQ: 20.335, a_model: 0.078
steps: 934975, episodes: 37400, mean episode reward: -575.71, reward: -7.464, time: 6.416, TD error: 5.917, c_model: 3.009, actorQ: 20.382, a_model: 0.078
steps: 937475, episodes: 37500, mean episode reward: -579.608, reward: -7.333, time: 6.351, TD error: 5.889, c_model: 2.995, actorQ: 20.431, a_model: 0.078
steps: 939975, episodes: 37600, mean episode reward: -570.488, reward: -7.395, time: 6.426, TD error: 5.86, c_model: 2.98, actorQ: 20.48, a_model: 0.078
steps: 942475, episodes: 37700, mean episode reward: -600.125, reward: -7.912, time: 6.391, TD error: 5.831, c_model: 2.966, actorQ: 20.529, a_model: 0.077
steps: 944975, episodes: 37800, mean episode reward: -579.772, reward: -7.554, time: 6.423, TD error: 5.805, c_model: 2.953, actorQ: 20.578, a_model: 0.077
steps: 947475, episodes: 37900, mean episode reward: -602.326, reward: -7.72, time: 6.542, TD error: 5.772, c_model: 2.936, actorQ: 20.638, a_model: 0.077
steps: 949975, episodes: 38000, mean episode reward: -597.402, reward: -7.709, time: 6.348, TD error: 5.742, c_model: 2.921, actorQ: 20.686, a_model: 0.077
steps: 952475, episodes: 38100, mean episode reward: -578.426, reward: -7.468, time: 6.899, TD error: 5.713, c_model: 2.906, actorQ: 20.731, a_model: 0.077
steps: 954975, episodes: 38200, mean episode reward: -591.871, reward: -7.646, time: 6.457, TD error: 5.687, c_model: 2.893, actorQ: 20.779, a_model: 0.077
steps: 957475, episodes: 38300, mean episode reward: -585.254, reward: -7.568, time: 6.378, TD error: 5.661, c_model: 2.88, actorQ: 20.825, a_model: 0.076
steps: 959975, episodes: 38400, mean episode reward: -607.067, reward: -7.921, time: 7.002, TD error: 5.635, c_model: 2.867, actorQ: 20.872, a_model: 0.076
steps: 962475, episodes: 38500, mean episode reward: -597.638, reward: -7.83, time: 6.656, TD error: 5.604, c_model: 2.851, actorQ: 20.93, a_model: 0.076
steps: 964975, episodes: 38600, mean episode reward: -592.061, reward: -7.754, time: 6.678, TD error: 5.578, c_model: 2.838, actorQ: 20.975, a_model: 0.076
steps: 967475, episodes: 38700, mean episode reward: -595.52, reward: -7.793, time: 6.613, TD error: 5.552, c_model: 2.825, actorQ: 21.022, a_model: 0.076
steps: 969975, episodes: 38800, mean episode reward: -582.319, reward: -7.607, time: 6.586, TD error: 5.527, c_model: 2.812, actorQ: 21.067, a_model: 0.076
steps: 972475, episodes: 38900, mean episode reward: -575.837, reward: -7.579, time: 6.647, TD error: 5.504, c_model: 2.8, actorQ: 21.113, a_model: 0.076
steps: 974975, episodes: 39000, mean episode reward: -590.837, reward: -7.491, time: 6.713, TD error: 5.481, c_model: 2.787, actorQ: 21.159, a_model: 0.075
steps: 977475, episodes: 39100, mean episode reward: -576.903, reward: -7.45, time: 6.873, TD error: 5.452, c_model: 2.772, actorQ: 21.216, a_model: 0.075
steps: 979975, episodes: 39200, mean episode reward: -579.438, reward: -7.334, time: 7.685, TD error: 5.428, c_model: 2.759, actorQ: 21.26, a_model: 0.075
steps: 982475, episodes: 39300, mean episode reward: -596.566, reward: -7.68, time: 6.82, TD error: 5.406, c_model: 2.747, actorQ: 21.304, a_model: 0.075
steps: 984975, episodes: 39400, mean episode reward: -595.729, reward: -7.818, time: 6.691, TD error: 5.383, c_model: 2.736, actorQ: 21.348, a_model: 0.075
steps: 987475, episodes: 39500, mean episode reward: -609.136, reward: -7.832, time: 6.425, TD error: 5.363, c_model: 2.724, actorQ: 21.393, a_model: 0.075
steps: 989975, episodes: 39600, mean episode reward: -578.059, reward: -7.447, time: 6.337, TD error: 5.341, c_model: 2.713, actorQ: 21.437, a_model: 0.075
steps: 992475, episodes: 39700, mean episode reward: -587.538, reward: -7.709, time: 6.479, TD error: 5.315, c_model: 2.699, actorQ: 21.491, a_model: 0.075
steps: 994975, episodes: 39800, mean episode reward: -599.318, reward: -7.588, time: 6.422, TD error: 5.296, c_model: 2.689, actorQ: 21.535, a_model: 0.074
steps: 997475, episodes: 39900, mean episode reward: -579.406, reward: -7.521, time: 6.291, TD error: 5.276, c_model: 2.678, actorQ: 21.578, a_model: 0.074
steps: 999975, episodes: 40000, mean episode reward: -602.159, reward: -7.729, time: 6.474, TD error: 5.256, c_model: 2.667, actorQ: 21.621, a_model: 0.074
steps: 1002475, episodes: 40100, mean episode reward: -589.099, reward: -7.474, time: 6.378, TD error: 5.239, c_model: 2.658, actorQ: 21.667, a_model: 0.074
steps: 1004975, episodes: 40200, mean episode reward: -572.021, reward: -7.263, time: 6.462, TD error: 5.219, c_model: 2.647, actorQ: 21.711, a_model: 0.074
steps: 1007475, episodes: 40300, mean episode reward: -572.84, reward: -7.499, time: 6.448, TD error: 5.193, c_model: 2.633, actorQ: 21.761, a_model: 0.074
steps: 1009975, episodes: 40400, mean episode reward: -588.77, reward: -7.654, time: 6.386, TD error: 5.175, c_model: 2.624, actorQ: 21.807, a_model: 0.074
steps: 1012475, episodes: 40500, mean episode reward: -587.818, reward: -7.915, time: 6.481, TD error: 5.156, c_model: 2.613, actorQ: 21.847, a_model: 0.074
steps: 1014975, episodes: 40600, mean episode reward: -582.679, reward: -7.554, time: 6.478, TD error: 5.139, c_model: 2.604, actorQ: 21.89, a_model: 0.074
steps: 1017475, episodes: 40700, mean episode reward: -582.156, reward: -7.598, time: 6.47, TD error: 5.123, c_model: 2.595, actorQ: 21.932, a_model: 0.074
steps: 1019975, episodes: 40800, mean episode reward: -589.725, reward: -7.722, time: 6.376, TD error: 5.107, c_model: 2.586, actorQ: 21.974, a_model: 0.074
steps: 1022475, episodes: 40900, mean episode reward: -606.071, reward: -7.848, time: 6.423, TD error: 5.083, c_model: 2.573, actorQ: 22.023, a_model: 0.073
steps: 1024975, episodes: 41000, mean episode reward: -582.355, reward: -7.407, time: 6.422, TD error: 5.064, c_model: 2.563, actorQ: 22.064, a_model: 0.073
steps: 1027475, episodes: 41100, mean episode reward: -578.773, reward: -7.356, time: 6.404, TD error: 5.046, c_model: 2.553, actorQ: 22.104, a_model: 0.073
steps: 1029975, episodes: 41200, mean episode reward: -589.283, reward: -7.626, time: 6.449, TD error: 5.03, c_model: 2.544, actorQ: 22.145, a_model: 0.073
steps: 1032475, episodes: 41300, mean episode reward: -579.12, reward: -7.575, time: 6.411, TD error: 5.015, c_model: 2.536, actorQ: 22.185, a_model: 0.073
steps: 1034975, episodes: 41400, mean episode reward: -600.741, reward: -7.801, time: 6.378, TD error: 4.999, c_model: 2.526, actorQ: 22.227, a_model: 0.073
steps: 1037475, episodes: 41500, mean episode reward: -600.321, reward: -7.995, time: 6.535, TD error: 4.98, c_model: 2.515, actorQ: 22.277, a_model: 0.073
steps: 1039975, episodes: 41600, mean episode reward: -598.2, reward: -7.719, time: 6.449, TD error: 4.967, c_model: 2.508, actorQ: 22.318, a_model: 0.073
steps: 1042475, episodes: 41700, mean episode reward: -591.311, reward: -7.727, time: 6.393, TD error: 4.953, c_model: 2.5, actorQ: 22.358, a_model: 0.073
steps: 1044975, episodes: 41800, mean episode reward: -580.139, reward: -7.535, time: 6.401, TD error: 4.938, c_model: 2.491, actorQ: 22.399, a_model: 0.073
steps: 1047475, episodes: 41900, mean episode reward: -574.14, reward: -7.559, time: 6.471, TD error: 4.922, c_model: 2.482, actorQ: 22.438, a_model: 0.073
steps: 1049975, episodes: 42000, mean episode reward: -599.773, reward: -7.866, time: 6.471, TD error: 4.908, c_model: 2.474, actorQ: 22.479, a_model: 0.073
steps: 1052475, episodes: 42100, mean episode reward: -593.144, reward: -7.609, time: 6.527, TD error: 4.891, c_model: 2.464, actorQ: 22.528, a_model: 0.073
steps: 1054975, episodes: 42200, mean episode reward: -611.531, reward: -7.978, time: 6.415, TD error: 4.876, c_model: 2.456, actorQ: 22.566, a_model: 0.073
steps: 1057475, episodes: 42300, mean episode reward: -584.698, reward: -7.571, time: 6.489, TD error: 4.861, c_model: 2.447, actorQ: 22.606, a_model: 0.073
steps: 1059975, episodes: 42400, mean episode reward: -574.695, reward: -7.353, time: 6.419, TD error: 4.85, c_model: 2.44, actorQ: 22.645, a_model: 0.073
steps: 1062475, episodes: 42500, mean episode reward: -566.418, reward: -7.32, time: 6.437, TD error: 4.839, c_model: 2.434, actorQ: 22.684, a_model: 0.072
steps: 1064975, episodes: 42600, mean episode reward: -585.343, reward: -7.532, time: 6.453, TD error: 4.825, c_model: 2.426, actorQ: 22.721, a_model: 0.072
steps: 1067475, episodes: 42700, mean episode reward: -565.85, reward: -7.496, time: 6.473, TD error: 4.811, c_model: 2.418, actorQ: 22.77, a_model: 0.072
steps: 1069975, episodes: 42800, mean episode reward: -584.926, reward: -7.476, time: 6.432, TD error: 4.799, c_model: 2.411, actorQ: 22.811, a_model: 0.072
steps: 1072475, episodes: 42900, mean episode reward: -568.403, reward: -7.274, time: 6.508, TD error: 4.786, c_model: 2.403, actorQ: 22.851, a_model: 0.072
steps: 1074975, episodes: 43000, mean episode reward: -566.091, reward: -7.329, time: 6.432, TD error: 4.773, c_model: 2.396, actorQ: 22.889, a_model: 0.072
steps: 1077475, episodes: 43100, mean episode reward: -591.667, reward: -7.638, time: 6.512, TD error: 4.76, c_model: 2.388, actorQ: 22.927, a_model: 0.072
steps: 1079975, episodes: 43200, mean episode reward: -581.708, reward: -7.42, time: 6.536, TD error: 4.747, c_model: 2.381, actorQ: 22.968, a_model: 0.072
steps: 1082475, episodes: 43300, mean episode reward: -570.911, reward: -7.339, time: 6.446, TD error: 4.732, c_model: 2.372, actorQ: 23.013, a_model: 0.072
steps: 1084975, episodes: 43400, mean episode reward: -573.037, reward: -7.34, time: 6.333, TD error: 4.719, c_model: 2.364, actorQ: 23.052, a_model: 0.072
steps: 1087475, episodes: 43500, mean episode reward: -582.172, reward: -7.521, time: 6.429, TD error: 4.705, c_model: 2.356, actorQ: 23.092, a_model: 0.072
steps: 1089975, episodes: 43600, mean episode reward: -579.564, reward: -7.57, time: 6.435, TD error: 4.695, c_model: 2.35, actorQ: 23.13, a_model: 0.072
steps: 1092475, episodes: 43700, mean episode reward: -575.017, reward: -7.404, time: 6.463, TD error: 4.684, c_model: 2.343, actorQ: 23.171, a_model: 0.072
steps: 1094975, episodes: 43800, mean episode reward: -575.274, reward: -7.289, time: 6.456, TD error: 4.674, c_model: 2.337, actorQ: 23.206, a_model: 0.072
steps: 1097475, episodes: 43900, mean episode reward: -594.326, reward: -7.613, time: 6.785, TD error: 4.661, c_model: 2.329, actorQ: 23.254, a_model: 0.072
steps: 1099975, episodes: 44000, mean episode reward: -578.988, reward: -7.481, time: 7.574, TD error: 4.65, c_model: 2.322, actorQ: 23.291, a_model: 0.072
steps: 1102475, episodes: 44100, mean episode reward: -578.901, reward: -7.446, time: 6.505, TD error: 4.641, c_model: 2.316, actorQ: 23.329, a_model: 0.072
steps: 1104975, episodes: 44200, mean episode reward: -594.834, reward: -7.736, time: 6.533, TD error: 4.629, c_model: 2.31, actorQ: 23.369, a_model: 0.072
steps: 1107475, episodes: 44300, mean episode reward: -575.942, reward: -7.314, time: 6.443, TD error: 4.62, c_model: 2.304, actorQ: 23.405, a_model: 0.072
steps: 1109975, episodes: 44400, mean episode reward: -577.858, reward: -7.447, time: 6.366, TD error: 4.611, c_model: 2.298, actorQ: 23.445, a_model: 0.072
steps: 1112475, episodes: 44500, mean episode reward: -578.605, reward: -7.465, time: 6.476, TD error: 4.596, c_model: 2.289, actorQ: 23.489, a_model: 0.071
steps: 1114975, episodes: 44600, mean episode reward: -586.069, reward: -7.585, time: 6.419, TD error: 4.587, c_model: 2.283, actorQ: 23.527, a_model: 0.071
steps: 1117475, episodes: 44700, mean episode reward: -562.621, reward: -7.259, time: 6.41, TD error: 4.578, c_model: 2.278, actorQ: 23.563, a_model: 0.071
steps: 1119975, episodes: 44800, mean episode reward: -576.43, reward: -7.368, time: 6.503, TD error: 4.569, c_model: 2.273, actorQ: 23.601, a_model: 0.071
steps: 1122475, episodes: 44900, mean episode reward: -587.174, reward: -7.55, time: 6.469, TD error: 4.56, c_model: 2.267, actorQ: 23.638, a_model: 0.071
steps: 1124975, episodes: 45000, mean episode reward: -581.059, reward: -7.334, time: 6.526, TD error: 4.55, c_model: 2.261, actorQ: 23.672, a_model: 0.071
steps: 1127475, episodes: 45100, mean episode reward: -577.506, reward: -7.573, time: 6.501, TD error: 4.538, c_model: 2.253, actorQ: 23.717, a_model: 0.071
steps: 1129975, episodes: 45200, mean episode reward: -583.114, reward: -7.596, time: 6.667, TD error: 4.531, c_model: 2.248, actorQ: 23.753, a_model: 0.071
steps: 1132475, episodes: 45300, mean episode reward: -563.803, reward: -7.349, time: 6.513, TD error: 4.524, c_model: 2.243, actorQ: 23.791, a_model: 0.071
steps: 1134975, episodes: 45400, mean episode reward: -597.383, reward: -7.641, time: 6.448, TD error: 4.514, c_model: 2.238, actorQ: 23.827, a_model: 0.071
steps: 1137475, episodes: 45500, mean episode reward: -583.57, reward: -7.401, time: 6.362, TD error: 4.507, c_model: 2.233, actorQ: 23.863, a_model: 0.071
steps: 1139975, episodes: 45600, mean episode reward: -579.039, reward: -7.43, time: 7.068, TD error: 4.498, c_model: 2.227, actorQ: 23.9, a_model: 0.071
steps: 1142475, episodes: 45700, mean episode reward: -581.612, reward: -7.444, time: 6.478, TD error: 4.487, c_model: 2.22, actorQ: 23.942, a_model: 0.071
steps: 1144975, episodes: 45800, mean episode reward: -595.71, reward: -7.697, time: 6.403, TD error: 4.477, c_model: 2.214, actorQ: 23.979, a_model: 0.071
steps: 1147475, episodes: 45900, mean episode reward: -586.889, reward: -7.555, time: 6.389, TD error: 4.469, c_model: 2.209, actorQ: 24.015, a_model: 0.071
steps: 1149975, episodes: 46000, mean episode reward: -573.958, reward: -7.405, time: 6.359, TD error: 4.46, c_model: 2.203, actorQ: 24.05, a_model: 0.071
steps: 1152475, episodes: 46100, mean episode reward: -574.214, reward: -7.294, time: 6.441, TD error: 4.454, c_model: 2.199, actorQ: 24.085, a_model: 0.071
steps: 1154975, episodes: 46200, mean episode reward: -580.223, reward: -7.41, time: 6.428, TD error: 4.446, c_model: 2.194, actorQ: 24.12, a_model: 0.071
steps: 1157475, episodes: 46300, mean episode reward: -570.956, reward: -7.33, time: 6.403, TD error: 4.437, c_model: 2.187, actorQ: 24.163, a_model: 0.071
steps: 1159975, episodes: 46400, mean episode reward: -579.936, reward: -7.588, time: 6.453, TD error: 4.428, c_model: 2.182, actorQ: 24.2, a_model: 0.071
steps: 1162475, episodes: 46500, mean episode reward: -579.481, reward: -7.386, time: 6.471, TD error: 4.421, c_model: 2.177, actorQ: 24.237, a_model: 0.071
steps: 1164975, episodes: 46600, mean episode reward: -561.569, reward: -7.25, time: 6.324, TD error: 4.414, c_model: 2.171, actorQ: 24.271, a_model: 0.071
steps: 1167475, episodes: 46700, mean episode reward: -587.418, reward: -7.541, time: 6.381, TD error: 4.407, c_model: 2.167, actorQ: 24.306, a_model: 0.071
steps: 1169975, episodes: 46800, mean episode reward: -595.829, reward: -7.567, time: 6.475, TD error: 4.399, c_model: 2.161, actorQ: 24.341, a_model: 0.071
steps: 1172475, episodes: 46900, mean episode reward: -593.522, reward: -7.557, time: 6.507, TD error: 4.391, c_model: 2.155, actorQ: 24.383, a_model: 0.071
steps: 1174975, episodes: 47000, mean episode reward: -580.482, reward: -7.455, time: 6.433, TD error: 4.384, c_model: 2.15, actorQ: 24.417, a_model: 0.07
steps: 1177475, episodes: 47100, mean episode reward: -580.7, reward: -7.456, time: 6.505, TD error: 4.378, c_model: 2.145, actorQ: 24.451, a_model: 0.07
steps: 1179975, episodes: 47200, mean episode reward: -597.152, reward: -7.696, time: 6.708, TD error: 4.372, c_model: 2.141, actorQ: 24.488, a_model: 0.07
steps: 1182475, episodes: 47300, mean episode reward: -595.153, reward: -7.811, time: 6.599, TD error: 4.365, c_model: 2.136, actorQ: 24.522, a_model: 0.07
steps: 1184975, episodes: 47400, mean episode reward: -556.919, reward: -7.099, time: 6.396, TD error: 4.358, c_model: 2.131, actorQ: 24.557, a_model: 0.07
steps: 1187475, episodes: 47500, mean episode reward: -608.298, reward: -7.78, time: 6.657, TD error: 4.352, c_model: 2.126, actorQ: 24.601, a_model: 0.07
steps: 1189975, episodes: 47600, mean episode reward: -570.179, reward: -7.394, time: 6.459, TD error: 4.347, c_model: 2.122, actorQ: 24.634, a_model: 0.07
steps: 1192475, episodes: 47700, mean episode reward: -583.019, reward: -7.471, time: 6.43, TD error: 4.34, c_model: 2.117, actorQ: 24.668, a_model: 0.07
steps: 1194975, episodes: 47800, mean episode reward: -566.642, reward: -7.255, time: 6.338, TD error: 4.335, c_model: 2.113, actorQ: 24.704, a_model: 0.07
steps: 1197475, episodes: 47900, mean episode reward: -582.366, reward: -7.498, time: 6.367, TD error: 4.332, c_model: 2.11, actorQ: 24.74, a_model: 0.07
steps: 1199975, episodes: 48000, mean episode reward: -581.828, reward: -7.502, time: 6.382, TD error: 4.327, c_model: 2.106, actorQ: 24.775, a_model: 0.07
steps: 1202475, episodes: 48100, mean episode reward: -567.453, reward: -7.153, time: 6.561, TD error: 4.318, c_model: 2.1, actorQ: 24.816, a_model: 0.07
steps: 1204975, episodes: 48200, mean episode reward: -587.9, reward: -7.434, time: 6.404, TD error: 4.313, c_model: 2.096, actorQ: 24.851, a_model: 0.07
steps: 1207475, episodes: 48300, mean episode reward: -595.382, reward: -7.404, time: 6.393, TD error: 4.307, c_model: 2.091, actorQ: 24.884, a_model: 0.07
steps: 1209975, episodes: 48400, mean episode reward: -575.176, reward: -7.392, time: 6.38, TD error: 4.302, c_model: 2.087, actorQ: 24.917, a_model: 0.07
steps: 1212475, episodes: 48500, mean episode reward: -573.116, reward: -7.369, time: 6.387, TD error: 4.297, c_model: 2.083, actorQ: 24.951, a_model: 0.07
steps: 1214975, episodes: 48600, mean episode reward: -575.763, reward: -7.298, time: 6.448, TD error: 4.292, c_model: 2.079, actorQ: 24.987, a_model: 0.07
steps: 1217475, episodes: 48700, mean episode reward: -555.863, reward: -7.15, time: 7.138, TD error: 4.286, c_model: 2.075, actorQ: 25.028, a_model: 0.07
steps: 1219975, episodes: 48800, mean episode reward: -579.563, reward: -7.486, time: 7.366, TD error: 4.28, c_model: 2.07, actorQ: 25.063, a_model: 0.07
steps: 1222475, episodes: 48900, mean episode reward: -556.862, reward: -7.037, time: 6.446, TD error: 4.274, c_model: 2.066, actorQ: 25.096, a_model: 0.07
